{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hila-chefer/Transformer-Explainability/blob/main/BERT_explainability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCdGaMuy56TA",
        "outputId": "8f802262-55eb-4366-b772-89c4756224b3"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/hila-chefer/Transformer-Explainability.git\n",
        "\n",
        "import os\n",
        "# os.chdir(f'./Transformer-Explainability')\n",
        "\n",
        "# !pip install -r requirements.txt\n",
        "# !pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDPnh4lofcNw",
        "outputId": "3d585bbc-ff3b-4a09-b5bf-57bb4d46e830"
      },
      "outputs": [],
      "source": [
        "# !pip install captum==0.6.0\n",
        "# !pip install matplotlib==3.3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4-XGl_Zw6Aht"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running on cuda:1\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "from BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
        "from BERT_explainability.modules.BERT.BertForSequenceClassification import BertForSequenceClassification\n",
        "from transformers import BertTokenizer\n",
        "from BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from captum.attr import visualization\n",
        "import torch\n",
        "import numpy as np\n",
        "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f'running on {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VakYjrkC6C3S"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\").to(device)\n",
        "model.eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
        "# initialize the explanations generator\n",
        "explanations = Generator(model)\n",
        "\n",
        "classifications = [\"NEGATIVE\", \"POSITIVE\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## interpret from 2nd paper\n",
        "writing the interpretation that is implemented in the followup paper with intent to compare between the two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attn_blocks=list(dict(model.bert.encoder.layer.named_children()).values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attn_blocks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'attn_blocks' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m attn_blocks[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mattention\u001b[39m.\u001b[39mself\u001b[39m.\u001b[39mattn\u001b[39m.\u001b[39mshape\n",
            "\u001b[0;31mNameError\u001b[0m: name 'attn_blocks' is not defined"
          ]
        }
      ],
      "source": [
        "attn_blocks[0].attention.self.attn.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(attn_blocks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_layer = 0\n",
        "\n",
        "def interpret(input_ids, model, attention_mask, start_layer_text=start_layer):\n",
        "    device = model.device\n",
        "    batch_size = input_ids.shape[0]\n",
        "    logits = model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
        "    probs = logits.softmax(dim=-1).detach().cpu().numpy()\n",
        "\n",
        "    # index = [i for i in range(batch_size)]\n",
        "    index = np.argmax(probs, axis=-1)\n",
        "    one_hot = np.zeros((logits.shape[0], logits.shape[1]), dtype=np.float32)\n",
        "    one_hot[torch.arange(logits.shape[0]), index] = 1\n",
        "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "    one_hot = torch.sum(one_hot.cuda(device) * logits)\n",
        "    model.zero_grad()\n",
        "\n",
        "    # text_attn_blocks = list(dict(model.transformer.resblocks.named_children()).values())\n",
        "    text_attn_blocks = list(dict(model.bert.encoder.layer.named_children()).values())\n",
        "\n",
        "    if start_layer_text == -1: \n",
        "      # calculate index of last layer \n",
        "      start_layer_text = len(text_attn_blocks) - 1\n",
        "\n",
        "    # num_tokens = text_attn_blocks[0].attn_probs.shape[-1]\n",
        "    num_tokens = text_attn_blocks[0].attention.self.attn.shape[-1]\n",
        "    R_text = torch.eye(num_tokens, num_tokens, dtype=text_attn_blocks[0].attention.self.attn.dtype).to(device)\n",
        "    R_text = R_text.unsqueeze(0).expand(batch_size, num_tokens, num_tokens)\n",
        "    for i, blk in enumerate(text_attn_blocks):\n",
        "        if i < start_layer_text:\n",
        "          continue\n",
        "        # grad = torch.autograd.grad(one_hot, [blk.attn_probs], retain_graph=True)[0].detach()\n",
        "        # cam = blk.attn_probs.detach()\n",
        "        grad = torch.autograd.grad(one_hot, [blk.attention.self.attn], retain_graph=True)[0].detach()\n",
        "        cam = blk.attention.self.attn.detach()\n",
        "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "        cam = grad * cam\n",
        "        cam = cam.reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
        "        cam = cam.clamp(min=0).mean(dim=1)\n",
        "        R_text = R_text + torch.bmm(cam, R_text)\n",
        "    expl = R_text\n",
        "    expl[:, 0, 0] = expl[:, 0].min()\n",
        "    # expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
        "    # expl = expl / expl.sum()\n",
        "    \n",
        "    \n",
        "   \n",
        "    return probs,expl[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('[CLS]', 0.0), ('this', 0.4532674252986908), ('movie', 0.4462960958480835), ('was', 0.21855813264846802), ('the', 0.2666281461715698), ('best', 0.5442412495613098), ('movie', 0.3071017563343048), ('i', 0.14080315828323364), ('have', 0.07363813370466232), ('ever', 0.09055276215076447), ('seen', 0.10591993480920792), ('!', 0.7834613919258118), ('some', 0.011827755719423294), ('scenes', 0.1388983577489853), ('were', 0.0), ('ridiculous', 0.10907001793384552), (',', 0.10549715906381607), ('but', 0.7055017948150635), ('acting', 0.8157413601875305), ('was', 0.4857148826122284), ('great', 1.0), ('.', 0.3004361093044281), ('[SEP]', 0.654569149017334)]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> i                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seen                    </font></mark><mark style=\"background-color: hsl(120, 75%, 61%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> scenes                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ridiculous                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 65%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> acting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> i                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seen                    </font></mark><mark style=\"background-color: hsl(120, 75%, 61%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> scenes                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ridiculous                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 65%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> acting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# encode a sentence\n",
        "text_batch = [\"This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.\"]\n",
        "encoding = tokenizer(text_batch, return_tensors='pt')\n",
        "input_ids = encoding['input_ids'].to(device)\n",
        "attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "# true class is positive - 1\n",
        "true_class = 1\n",
        "\n",
        "probs, expl = interpret(input_ids,model,attention_mask,start_layer_text=0)\n",
        "expl = expl[0]\n",
        "expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
        "# expl = expl / expl.sum()\n",
        "classification = probs.argmax(axis=-1).item()\n",
        "# get class name\n",
        "class_name = classifications[classification]\n",
        "# if the classification is negative, higher explanation scores are more negative\n",
        "# flip for visualization\n",
        "if class_name == \"NEGATIVE\":\n",
        "  expl *= (-1)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
        "print([(tokens[i], expl[i].item()) for i in range(len(tokens))])\n",
        "vis_data_records = [visualization.VisualizationDataRecord(\n",
        "                                expl,\n",
        "                                probs[0][classification],\n",
        "                                classification,\n",
        "                                true_class,\n",
        "                                true_class,\n",
        "                                1,       \n",
        "                                tokens,\n",
        "                                1)]\n",
        "visualization.visualize_text(vis_data_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0005, 0.0023, 0.0023, 0.0014, 0.0016, 0.0026, 0.0017, 0.0011, 0.0008,\n",
              "        0.0009, 0.0009, 0.0036, 0.0006, 0.0011, 0.0005, 0.0010, 0.0009, 0.0033,\n",
              "        0.0037, 0.0024, 0.0044, 0.0017, 0.0031], device='cuda:1')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "expl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jGRp376FPOvV"
      },
      "source": [
        "## original paper \n",
        "### Positive sentiment example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([23])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "expl.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 23])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "uSLZtv546H2z",
        "outputId": "26712e90-0b77-40b0-a908-fef13dd88bcd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gkoren2/work/git/clones/transformers/src/transformers/modeling_utils.py:866: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/home/gkoren2/work/git/clones/transformers/src/transformers/modeling_utils.py:866: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('[CLS]', 0.0), ('this', 0.4272907078266144), ('movie', 0.30951225757598877), ('was', 0.268684446811676), ('the', 0.339260071516037), ('best', 0.6288697123527527), ('movie', 0.28607645630836487), ('i', 0.1867041438817978), ('have', 0.10146278887987137), ('ever', 0.14219637215137482), ('seen', 0.19010096788406372), ('!', 0.5949175953865051), ('some', 0.0040426114574074745), ('scenes', 0.0334150567650795), ('were', 0.01868431642651558), ('ridiculous', 0.018948297947645187), (',', 0.0), ('but', 0.42987316846847534), ('acting', 0.43847790360450745), ('was', 0.5003233551979065), ('great', 1.0), ('.', 0.014387628063559532), ('[SEP]', 0.08716049790382385)]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> i                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seen                    </font></mark><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> scenes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ridiculous                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> acting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 69%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> i                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seen                    </font></mark><mark style=\"background-color: hsl(120, 75%, 71%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> scenes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ridiculous                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> acting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# encode a sentence\n",
        "text_batch = [\"This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.\"]\n",
        "encoding = tokenizer(text_batch, return_tensors='pt')\n",
        "input_ids = encoding['input_ids'].to(device)\n",
        "attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "# true class is positive - 1\n",
        "true_class = 1\n",
        "\n",
        "# generate an explanation for the input\n",
        "expl = explanations.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=0)[0]\n",
        "# normalize scores\n",
        "expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
        "\n",
        "# get the model classification\n",
        "output = torch.nn.functional.softmax(model(input_ids=input_ids, attention_mask=attention_mask)[0], dim=-1)\n",
        "classification = output.argmax(dim=-1).item()\n",
        "# get class name\n",
        "class_name = classifications[classification]\n",
        "# if the classification is negative, higher explanation scores are more negative\n",
        "# flip for visualization\n",
        "if class_name == \"NEGATIVE\":\n",
        "  expl *= (-1)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
        "print([(tokens[i], expl[i].item()) for i in range(len(tokens))])\n",
        "vis_data_records = [visualization.VisualizationDataRecord(\n",
        "                                expl,\n",
        "                                output[0][classification],\n",
        "                                classification,\n",
        "                                true_class,\n",
        "                                true_class,\n",
        "                                1,       \n",
        "                                tokens,\n",
        "                                1)]\n",
        "visualization.visualize_text(vis_data_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO_k1BtSPVt3"
      },
      "source": [
        "#Negative sentiment example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "gD4xcvovI1KI",
        "outputId": "e4a50a94-da4c-460e-b602-052b09cec28f"
      },
      "outputs": [],
      "source": [
        "# encode a sentence\n",
        "text_batch = [\"I really didn't like this movie. Some of the actors were good, but overall the movie was boring.\"]\n",
        "encoding = tokenizer(text_batch, return_tensors='pt')\n",
        "input_ids = encoding['input_ids'].to(device)\n",
        "attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "# generate an explanation for the input\n",
        "expl = explanations.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=0)[0]\n",
        "# normalize scores\n",
        "expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
        "\n",
        "# get the model classification\n",
        "output = torch.nn.functional.softmax(model(input_ids=input_ids, attention_mask=attention_mask)[0], dim=-1)\n",
        "classification = output.argmax(dim=-1).item()\n",
        "# get class name\n",
        "class_name = classifications[classification]\n",
        "# if the classification is negative, higher explanation scores are more negative\n",
        "# flip for visualization\n",
        "if class_name == \"NEGATIVE\":\n",
        "  expl *= (-1)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
        "print([(tokens[i], expl[i].item()) for i in range(len(tokens))])\n",
        "vis_data_records = [visualization.VisualizationDataRecord(\n",
        "                                expl,\n",
        "                                output[0][classification],\n",
        "                                classification,\n",
        "                                1,\n",
        "                                1,\n",
        "                                1,       \n",
        "                                tokens,\n",
        "                                1)]\n",
        "visualization.visualize_text(vis_data_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUn2_SMPNG-Y"
      },
      "source": [
        "# Choosing class for visualization example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "VQVmMFnzhPoV",
        "outputId": "26a43f8a-340c-4821-b39c-80105a565810"
      },
      "outputs": [],
      "source": [
        "# encode a sentence\n",
        "text_batch = [\"I hate that I love you.\"]\n",
        "encoding = tokenizer(text_batch, return_tensors='pt')\n",
        "input_ids = encoding['input_ids'].to(device)\n",
        "attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "# true class is positive - 1\n",
        "true_class = 1\n",
        "\n",
        "# generate an explanation for the input\n",
        "target_class = 0\n",
        "expl = explanations.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=11, index=target_class)[0]\n",
        "# normalize scores\n",
        "expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
        "\n",
        "# get the model classification\n",
        "output = torch.nn.functional.softmax(model(input_ids=input_ids, attention_mask=attention_mask)[0], dim=-1)\n",
        "\n",
        "# get class name\n",
        "class_name = classifications[target_class]\n",
        "# if the classification is negative, higher explanation scores are more negative\n",
        "# flip for visualization\n",
        "if class_name == \"NEGATIVE\":\n",
        "  expl *= (-1)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
        "print([(tokens[i], expl[i].item()) for i in range(len(tokens))])\n",
        "vis_data_records = [visualization.VisualizationDataRecord(\n",
        "                                expl,\n",
        "                                output[0][classification],\n",
        "                                classification,\n",
        "                                true_class,\n",
        "                                true_class,\n",
        "                                1,       \n",
        "                                tokens,\n",
        "                                1)]\n",
        "visualization.visualize_text(vis_data_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "WiQAWw0-imCg",
        "outputId": "a8c66996-dcd0-4132-a8b0-2346d9bf9c7b"
      },
      "outputs": [],
      "source": [
        "# encode a sentence\n",
        "text_batch = [\"I hate that I love you.\"]\n",
        "encoding = tokenizer(text_batch, return_tensors='pt')\n",
        "input_ids = encoding['input_ids'].to(device)\n",
        "attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "# true class is positive - 1\n",
        "true_class = 1\n",
        "\n",
        "# generate an explanation for the input\n",
        "target_class = 1\n",
        "expl = explanations.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=11, index=target_class)[0]\n",
        "# normalize scores\n",
        "expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
        "\n",
        "# get the model classification\n",
        "output = torch.nn.functional.softmax(model(input_ids=input_ids, attention_mask=attention_mask)[0], dim=-1)\n",
        "\n",
        "# get class name\n",
        "class_name = classifications[target_class]\n",
        "# if the classification is negative, higher explanation scores are more negative\n",
        "# flip for visualization\n",
        "if class_name == \"NEGATIVE\":\n",
        "  expl *= (-1)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
        "print([(tokens[i], expl[i].item()) for i in range(len(tokens))])\n",
        "vis_data_records = [visualization.VisualizationDataRecord(\n",
        "                                expl,\n",
        "                                output[0][classification],\n",
        "                                classification,\n",
        "                                true_class,\n",
        "                                true_class,\n",
        "                                1,       \n",
        "                                tokens,\n",
        "                                1)]\n",
        "visualization.visualize_text(vis_data_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOm8dIRrumd5XNcc+fntVA5",
      "include_colab_link": true,
      "name": "BERT-explainability.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
